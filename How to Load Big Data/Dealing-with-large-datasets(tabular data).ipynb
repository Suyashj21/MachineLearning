{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing-with-large-datasets(tabular data) in Normal System (16 GB)\n",
    "https://www.kaggle.com/yashvi/dealing-with-large-datasets-tabular-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 1 - Changing the size of datatypes\n",
    "train.info(memory_usage=\"deep\") # to check table details with Memory usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"Transaction unique identifier\": \"category\",\n",
    "    \"Price\": \"int32\",\n",
    "    \"Property Type\": \"category\",\n",
    "    \"Old/New\": \"category\",\n",
    "    \"Duration\": \"category\",\n",
    "    \"Town/City\": \"category\",\n",
    "    \"District\": \"category\",\n",
    "    \"Country\": \"category\",\n",
    "    \"County\": \"category\",\n",
    "    \"PPDCategory Type\": \"category\",\n",
    "    \"Record Status - monthly file only\": \"category\"\n",
    "}\n",
    "\n",
    "train = pd.read_csv(\"../input/uk-housing-prices-paid/price_paid_records.csv\", dtype=dtypes)\n",
    "train[\"Date of Transfer\"] = pd.to_datetime(train[\"Date of Transfer\"])\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"\", dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Down cast the Dtype using function:\n",
    "def downcast_dtypes(df):\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n",
    "    object_cols = [ c for c in df if df[c].dtype =='object']\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int16)\n",
    "    df[object_cols] = df[object_cols].astype(\"category\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2 - Reading data in chunks\n",
    "data_chunk=pd.read_csv('../input/uk-housing-prices-paid/price_paid_records.csv',chunksize=100000)\n",
    "\n",
    "#storing the chunks after iterating from chunk object\n",
    "chunk_data=[chunk for chunk in data_chunk]\n",
    "train=pd.concat(chunk_data)\n",
    "\n",
    "# del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3 - Use Dask\n",
    "- Dask is a flexible library for parallel computing in Python\n",
    "- Dask = Features of pandas + ( performance and scalability)\n",
    "\n",
    ">Use when :-\n",
    "Manipulating large datasets, even when those datasets don’t fit in memory\n",
    "Accelerating long computations by using many cores\n",
    "Distributed computing on large datasets with standard Pandas operations like groupby, join, and time series computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# gc.collect()\n",
    "import dask.dataframe as dd\n",
    "train = dd.read_csv(\"../input/uk-housing-prices-paid/price_paid_records.csv\", dtype=dtypes)\n",
    "train.info(memory_usage=\"deep\")\n",
    "\n",
    "# Calculation format:\n",
    "train.Price.mean().compute()\n",
    "train.Price[train.Price>10000].compute()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Method 4 : Using Datatable\n",
    "Datatable is a python library for manipulating tabular data. It supports out-of-memory datasets, multi-threaded data processing, and flexible API.\n",
    "\n",
    "Similar to the R’s data.table\n",
    "It is a toolkit for performing big data (up to 100GB) operations on a single-node machine, at the maximum possible speed.\n",
    "\n",
    "# Install datatable\n",
    "# !pip install datatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datatable as dt\n",
    "train = dt.fread(\"../input/uk-housing-prices-paid/price_paid_records.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Method 5 : Using cuDF\n",
    "cuDf is GPU equivalent to pandas . cuDF is a package within the RAPIDS ecosystem that allows data scientists to easily migrate their existing Pandas workflows from CPU to GPU, where computations can leverage the immense parallelization that GPUs provide.\n",
    "# !pip3 install cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import cudf\n",
    "train = cudf.read_csv('../input/uk-housing-prices-paid/price_paid_records.csv')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 6: Always delete unused variables ( this will save lot of space )\n",
    "\n",
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 7: Use Debug mode when doing feature engineering\n",
    "debug = True\n",
    "if debug:\n",
    "    train = pd.read_csv('../input/uk-housing-prices-paid/price_paid_records.csv',nrows=10000 , dtype=dtypes)\n",
    "else:\n",
    "    train = pd.read_csv('../input/uk-housing-prices-paid/price_paid_records.csv', dtype=dtypes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 8: Saving dataframes/objects as pickle files for faster loading\n",
    "train.to_pickle(\"train.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
