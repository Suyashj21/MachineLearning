{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/reinforcement-learning-tutorial-part-3-basic-deep-q-learning-186164c3bf4\n",
    "\n",
    "# Single experience = (old state, action, reward, new state)\n",
    "#Training our model with a single experience:\n",
    "1. Let the model estimate Q values of the old state\n",
    "2. Let the model estimate Q values of the new state\n",
    "3. Calculate the new target Q value for the action, using the known reward\n",
    "\n",
    "Train the model with input = (old state), output = (target Q values)\n",
    "\n",
    "Note: Our network doesnâ€™t get (state, action) as input like the Q-learning function Q(s,a) does. \n",
    "      This is because we are not replicating Q-learning as a whole, just the Q-table. \n",
    "      The input is just the state and the output is Q-values for all possible actions (forward, backward) for that state.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enums import *\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class DeepGambler:\n",
    "    def __init__(self, learning_rate=0.1, discount=0.95, \n",
    "                 exploration_rate=0.1, iterations=10000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_delta = 1.0 / iterations\n",
    "        \n",
    "        #inputs has five neurons, each represents single game state (0-4)\n",
    "        self.input_count = 5\n",
    "        # Output is two neurons, each represents Q-Value for Action (Forward and Backword)\n",
    "        self.output_count = 2\n",
    "        \n",
    "        self.session = tf.session()\n",
    "        self.define_model()\n",
    "        self.session.run(self.initializer)\n",
    "        \n",
    "    def define_model(self):\n",
    "        \n",
    "        # Input is an array of 5 items (state one-hot)\n",
    "        # Input is 2-dimensional, due to possibility of batched training data\n",
    "        # NOTE: In this example we assume no batching.         \n",
    "        self.model_input = tf.placeholder(dtype=tf.float32, shape=[None, self.input_count])\n",
    "        fc1 = tf.layers.dense(self.model_input, 16, activation=tf.sigmoid, kernel_initializer=tf.contant_initializer(np.zeros((self.input_count,16))))\n",
    "        fc2 = tf.layers.dense(fc1,16, activation=tf.sigmoid, kernel_initializer=tf.constant_initializer(np.zeros((16,self.output_count))))\n",
    "        \n",
    "        self.model_output = tf.layers.dense(fc2, self.output_count)\n",
    "        \n",
    "        self.target_output= tf.placeholder(shape=[None, self.output_count], dtype=tf.float32)\n",
    "        \n",
    "        loss - tf.losses.mean_squared_error(self.target_output, self.model_output)\n",
    "        \n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "        \n",
    "        self.initializer = tf.global_variables_initializer()\n",
    "        \n",
    "    def get_Q(self, state):\n",
    "        # Model input: Single state represented by array of 5 items (state one-hot)\n",
    "        # Model output: Array of Q values for single state        \n",
    "        return self.session.run(self.model_output, feed_dict={self.model_input: self.to_one_hot(state)})[0]\n",
    "    \n",
    "    # Turn state into 2d one-hot tensor\n",
    "    # Example: 3 -> [[0,0,0,1,0]]    \n",
    "    def to_one_hot(self, state):\n",
    "        one_hot = np.zeros((1,5))\n",
    "        one_hot[0, [state]] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def get_next_action(self, state):\n",
    "        if random.random() > self.exploration_rate:\n",
    "            return self.greedy_action(state)\n",
    "        else:\n",
    "            return self.random_action()\n",
    "        \n",
    "    def greedy_action(self, state):\n",
    "        # argmax picks the higher Q-value and returns the index (FORWARD=0, BACKWARD=1)        \n",
    "        retun np.argmax(self.get_Q(state))\n",
    "        \n",
    "\n",
    "    def random_action(self):\n",
    "        return FORWARD if random.random() < 0.5 else BACKWORD\n",
    "    \n",
    "    def train(self, old_state, action, reward, new_state):\n",
    "        \n",
    "        old_state_Q_values = self.get_Q(old_state)\n",
    "        \n",
    "        new_state_Q_values = self.get_Q(new_state)\n",
    "        \n",
    "        old_state_Q_values[action] = reward + self.dicount * np.amex(new_state_Q_values)\n",
    "        \n",
    "        #setup training data\n",
    "        training_input = self.to_one_hot(old_state)\n",
    "        target_output = [old_state_Q_values]\n",
    "        training_data = {self.model_input: training_input, self.target_output}\n",
    "        \n",
    "        #Train\n",
    "        self.session.run(self.optimizer, feed_dict=training_data)\n",
    "        \n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        #Train our model with new data\n",
    "        self.train(old_state, action, reward, new_state)\n",
    "        \n",
    "        #Finally shift our exploration_rate toward zero(less gambling)\n",
    "        if self.exploration_rate > 0:\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
