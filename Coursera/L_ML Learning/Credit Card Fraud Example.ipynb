{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good example for recall function for imbalanced data.\n",
    "#https://www.kaggle.com/gargmanish/how-to-handle-imbalance-data-study-in-detail\n",
    "# For data download: https://www.kaggle.com/mlg-ulb/creditcardfraud/downloads/creditcard.csv/3\n",
    "# Use given link to leanr technique for imbalanced data classification:\n",
    "#https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html\n",
    "# https://www.kaggle.com/themlguy/undersample-and-oversample-approach-explored\n",
    "\n",
    "\n",
    "#Tuning the mode: https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65\n",
    "#For ROC Curve: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/ \n",
    "\n",
    "#In this case, other alternative evaluation metrics can be applied such as:\n",
    "\n",
    "#----Precision/Specificity: how many selected instances are relevant.\n",
    "#----Recall/Sensitivity: how many relevant instances are selected.\n",
    "#----F1 score: harmonic mean of precision and recall.\n",
    "#----MCC: correlation coefficient between the observed and predicted binary classifications.\n",
    "#----AUC: relation between true-positive rate and false positive rate.\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"AP_Sample_data1.csv\")\n",
    "\n",
    "df = df.drop(['unique_id_popln', 'pcn_no','cust_xref_id','cust_xref_id_1'], axis=1)\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Converting object columns into numeric:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for check in df.columns:\n",
    "    if df[check].dtype == 'object':\n",
    "        df[check].fillna(\"?\", inplace=True)\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(df[check].unique())\n",
    "        df[check] = label_encoder.transform(df[check]) \n",
    "\n",
    "# use this to check null or nan values in dataset\n",
    "# df.isna().sum()\n",
    "df.fillna(0,inplace=True)\n",
    "df.head()\n",
    "\n",
    "#Normalize dataset:\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(df)\n",
    "df_normalized = pd.DataFrame(np_scaled)\n",
    "df_normalized.head()\n",
    "\n",
    "# Giving Column name (Lost during normalization)\n",
    "df_normalized.columns =df.columns\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------- Starting Underpopulation Process:--------------------------->>>\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\n",
    "\n",
    "#------------------ Process for Underpopulation No Flag Data --------------------------->>>>>>>>>> \n",
    "def generatePerformanceReport(clf, x_train, y_train,x_test, y_test, bool_):\n",
    "    if bool_ == True:\n",
    "        clf.fit(x_train,y_train.values.ravel())\n",
    "    pred = clf.predict(x_test)\n",
    "    cnf_matrix = confusion_matrix(y_test,pred)\n",
    "    TN,FP,FN,TP = cnf_matrix.ravel()\n",
    "    \n",
    "    print('------------------------------------')\n",
    "    print('Lenth of training data:',len(x_train))\n",
    "    print('Lenth of test data:',len(x_test))\n",
    "    print('------------------------------------')\n",
    "    print('True positives:',TP)\n",
    "    print('True negatives:',TN)\n",
    "    print('False positives:',FP)\n",
    "    print('False negatives:',FN)\n",
    "    \n",
    "    print('--------Classification report-----------')\n",
    "    print(classification_report(y_test,pred))\n",
    "\n",
    "normal_indices = df_normalized[df_normalized['auto_sltn_opp_flag']==0].index  \n",
    "ap_auto_indices = df_normalized[df_normalized['auto_sltn_opp_flag']==1].index\n",
    "\n",
    "#------------- Calling all model and checking underpopulated data in different ratio ---------->>>>>\n",
    "\n",
    "for i in range(1,4):\n",
    "    normal_sample_data = np.array(np.random.choice(normal_indices, i*len(ap_auto_indices),replace=False))\n",
    "    \n",
    "    undersampled_data = np.concatenate([ap_auto_indices, normal_sample_data])\n",
    "    \n",
    "    undersampled_data = df_normalized.iloc[undersampled_data]\n",
    "    \n",
    "    print('length of undersample data', len(undersampled_data))\n",
    "    \n",
    "    print('% of AP Auto Opportunity in undersampled data ',len(undersampled_data.loc[undersampled_data['auto_sltn_opp_flag']==1])/len(undersampled_data)) \n",
    "    \n",
    "    # Get feature and label data:\n",
    "    feature_data = undersampled_data.loc[:,undersampled_data.columns!='auto_sltn_opp_flag']\n",
    "    label_data = undersampled_data.loc[:,undersampled_data.columns=='auto_sltn_opp_flag']\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(feature_data, label_data, test_size=0.30)\n",
    " \n",
    "    # Defining loop for Models:------------------\n",
    "    for j in [LogisticRegression(),SVC(),RandomForestClassifier(n_estimators=100)]:\n",
    "        clf=j\n",
    "        print(j)\n",
    "        generatePerformanceReport(clf, x_train, y_train, x_test, y_test, True)\n",
    "        \n",
    "        remaining_indices = [i for i in df_normalized.index if i not in undersampled_data.index]\n",
    "        \n",
    "        testdf = df_normalized.iloc[remaining_indices]\n",
    "        testdf_label = df_normalized.loc[:, testdf.columns=='auto_sltn_opp_flag']\n",
    "        \n",
    "        testdf_features=df_normalized.loc[:,testdf.columns!='auto_sltn_opp_flag']\n",
    "        \n",
    "        generatePerformanceReport(clf,x_train,y_train, testdf_features, testdf_label, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------- Another process to check the result for unbalanced data:----- \n",
    "#oversampled_data------------------------------------>>>>>>>>\n",
    "normal_sampled_indices=df_normalized.loc[df_normalized['auto_sltn_opp_flag']==0].index\n",
    "oversampled_data=df_normalized.iloc[normal_sampled_indices]\n",
    "ap_auto_data=df_normalized.loc[df_normalized['auto_sltn_opp_flag']==1]\n",
    "oversampled_data=oversampled_data.append([ap_auto_data]*11, ignore_index=True)\n",
    "\n",
    "print('length of oversampled_data data ', len(oversampled_data))\n",
    "print('% of AP Auto opportunity in oversampled_data data ',len(oversampled_data.loc[oversampled_data['auto_sltn_opp_flag']==1])/len(oversampled_data))\n",
    "\n",
    "\n",
    "#get feature and label data\n",
    "feature_data=oversampled_data.loc[:,oversampled_data.columns!='auto_sltn_opp_flag']\n",
    "label_data=oversampled_data.loc[:,oversampled_data.columns=='auto_sltn_opp_flag']\n",
    "X_train, X_test, y_train, y_test=train_test_split(feature_data,label_data,test_size=0.30)\n",
    "for j in [LogisticRegression(),RandomForestClassifier(n_estimators=100)]:\n",
    "    clf=j\n",
    "    print(j)\n",
    "    generatePerformanceReport(clf,X_train,y_train,X_test,y_test,True)\n",
    "    #the above code classifies X_test which is part of undersampled data\n",
    "    #now, let us consider the remaining rows of dataset and use that as test set\n",
    "    remaining_indices=[i for i in df_normalized.index  if i not in oversampled_data.index]\n",
    "    testdf=df_normalized.iloc[remaining_indices]\n",
    "    testdf_label=df_normalized.loc[:,testdf.columns=='auto_sltn_opp_flag']\n",
    "    testdf_feature=df_normalized.loc[:,testdf.columns!='auto_sltn_opp_flag']\n",
    "    generatePerformanceReport(clf,X_train,y_train,testdf_feature,testdf_label,False)\n",
    "    \n",
    "# RandomForestClassifier working fine in Oversampl method for AP Automation unbalanced data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
